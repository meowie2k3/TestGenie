%% TODO: Make sure to use \textbf or \textit for highlighting keywords, and \cite{} to cite the corresponding quotations
\section{Unit test generator}

\paragraph{Evolution of Test Generation Approaches.} Software testing has evolved significantly over the decades, transitioning from purely manual testing to increasingly automated approaches. The earliest automated test generation methods emerged in the 1970s with simple input-output validation techniques~\cite{TestHistory}. By the 1990s, more sophisticated approaches began to appear, including symbolic execution and model-based testing. The 2000s saw the rise of search-based software testing (SBST), which applies metaheuristic search techniques to generate test cases that satisfy specific coverage criteria~\cite{SearchBasedSurvey}. In parallel, constraint-based testing evolved to leverage constraint solvers for generating test inputs that exercise specific code paths. Random testing, despite its simplicity, has remained relevant due to its ability to discover unexpected failures with minimal assumptions about the system under test.

\hspace{0.5cm} These traditional approaches have dominated the automated testing landscape until recently, when the emergence of advanced artificial intelligence techniques, particularly Large Language Models (LLMs), introduced a paradigm shift in how test generation can be approached. Unlike previous methods that relied on explicit algorithms or search strategies, LLM-based approaches leverage patterns learned from vast corpora of code to generate tests that more closely resemble those written by human developers.

\paragraph{LLMs approach compared to formulated approach.} To accurately give test case with correct syntax, I have researched some techniques that can handle different frameworks with just one centrialized system. There is a research that compares the performance of some common approaches including search-based, constraint-based and random-based. Tests generated by these methods frequently lack meaningful structure or descriptive naming conventions, making them difficult for developers to interpret and modify ~\cite{UnitTest}. This limitation can hinder their practical usability, particularly in dynamic and iterative development environments.

\hspace{0.5cm} In contrast, test case generation using Large Language Models (LLMs) offers a more intuitive and human-aligned approach~\cite{UnitTest}. LLMs, trained on vast amounts of programming-related data, possess the capability to generate test cases that not only adhere to syntactical correctness but also align closely with human developers' intentions and coding practices. This alignment results in unit tests that are more readable, contextually relevant, and easier to understand. Developers can quickly adjust and refine these tests as needed, enhancing their utility in real-world scenarios.

\hspace{0.5cm} Moreover, the flexibility of LLMs enables them to adapt seamlessly to various programming languages and frameworks, providing a centralized solution for diverse development ecosystems. While traditional approaches may produce marginally higher percentages of technically correct test cases, they often lack the usability and adaptability that LLM-based methods provide. As a result, services leveraging LLMs for test generation consistently receive more favorable user feedback due to their focus on developer experience, ease of use, and alignment with real-world development workflows.

\paragraph{Quantitative Analysis of Test Generation Approaches.} Recent empirical studies have provided quantitative evidence comparing traditional and LLM-based test generation approaches across multiple dimensions. According to a comprehensive benchmark study by Watson et al.~\cite{TestBenchmark}, test cases generated by LLMs achieved an average of 76\% code coverage compared to 82\% from specialized constraint-based tools. However, when measuring test suite maintainability using the Test Maintainability Index (TMI), LLM-generated tests scored significantly higher with an average score of 68 compared to 42 for traditional methods. This highlights a fundamental trade-off between technical perfection and practical usability.

\hspace{0.5cm} Table~\ref{tab:test-gen-comparison} provides a comparative analysis of different test generation approaches based on several key metrics, synthesized from multiple research studies~\cite{UnitTest, TestBenchmark, LLMTestComparison}.

\begin{table}[ht]
	\centering
	\caption{Comparison of Test Generation Approaches}
	\label{tab:test-gen-comparison}
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{lcccc}
			\hline
			\textbf{Metric} & \textbf{Search-based} & \textbf{Constraint-based} & \textbf{Random-based} & \textbf{LLM-based} \\ \hline
			Code Coverage & High (75-85\%) & Very High (80-90\%) & Medium (50-65\%) & High (70-80\%) \\
			Test Readability & Low & Medium & Very Low & Very High \\
			Naming Conventions & Poor & Moderate & Poor & Good to Excellent \\
			Framework Adaptability & Low & Low & Medium & High \\
			Edge Case Detection & Medium & High & Medium-High & Medium \\
			Maintenance Effort & High & Medium & High & Low \\
			Generation Speed & Fast & Slow & Very Fast & Medium \\
			Developer Satisfaction & Low & Medium & Low & High \\
			\hline
		\end{tabular}%
	}
\end{table}

\hspace{0.5cm} The data reveals that while traditional methods may excel in specific technical metrics like edge case detection (particularly constraint-based approaches), LLM-based methods offer a more balanced profile with particular strengths in human-centric metrics such as readability and maintainability. This balance makes LLM-based approaches especially suitable for industrial applications where developer productivity and code maintainability are primary concerns.

\paragraph{Disadvantages of  LLMs.}One of the most significant challenges is their propensity to generate hallucinations, where the model produces incorrect or fabricated outputs that lack grounding in factual data. This issue is particularly critical in tasks requiring precision, such as author attribution. For instance, research introducing the Simple Hallucination Index (SHI) revealed that even advanced LLMs like Mixtral 8x7B, LLaMA-2-13B, and Gemma-7B suffered from hallucinations, with Mixtral 8x7B achieving an SHI as high as 0.87 for certain datasets ~\cite{LLMLimitations}. These hallucinations undermine the reliability and trustworthiness of LLMs, especially in contexts where factual accuracy is crucial. 

\hspace{0.5cm} Another drawback of LLMs is their lack of transparency in decision-making. These models function as black boxes, providing little insight into the reasoning behind their outputs ~\cite{LLMLimitations}. This opacity complicates the debugging process and limits the ability to verify results, which is particularly problematic in applications requiring a high degree of explainability. Additionally, LLMs are highly dependent on the quality and diversity of their training data. Biases or inaccuracies present in the data can result in outputs that reinforce those biases or produce flawed results. Moreover, while these models excel at generating output based on their training corpus, they often struggle to generalize effectively when faced with novel or unseen cases.

\paragraph{LLM Limitations in Testing Contexts.} While general LLM limitations are well-documented, their specific impact on test generation presents unique challenges. Test generation requires a deep understanding of program semantics, expected behaviors, and framework-specific testing conventions. Wang et al.~\cite{LLMTestingLimitations} identified several testing-specific limitations in their comprehensive evaluation of LLM-based testing tools:

\hspace{0.5cm} First, LLMs frequently generate \textbf{syntactically valid but semantically incorrect} tests, particularly when dealing with complex object relationships or state-dependent behaviors. In their study, approximately 32\% of generated tests contained semantic errors despite being syntactically correct. Second, LLMs demonstrate \textbf{inconsistent mocking behavior}, struggling to correctly identify which components should be mocked in unit tests and how to implement those mocks appropriately. Third, there exists a \textbf{framework understanding gap}, where LLMs may mix testing conventions from different frameworks or misapply testing patterns. For Flutter specifically, Li and Zhang~\cite{FlutterTestingChallenges} found that LLMs often confused widget testing and integration testing approaches, leading to inefficient or incorrect test implementations.

\hspace{0.5cm} These limitations highlight the need for specialized approaches when applying LLMs to test generation tasks. Solutions proposed in recent literature include fine-tuning models on framework-specific testing examples, implementing post-processing validation steps, and incorporating human feedback loops to refine and correct generated tests~\cite{LLMTestEnhancements}.

\section{Understanding Business Logic}

\paragraph{The concept of Business Logic.} An industry's business logic can be seen as a description of a number of basic conditions or circumstances that make up important starting points for understanding an established business and its conditions for change~\cite{BusinessRules}. It encodes the real-world policies, procedures, and processes that govern how data is created, managed, and manipulated in a way that aligns with the objectives of the organization. Business logic acts as the foundation for decision-making and operational tasks, ensuring that the software performs actions that mirror the intended business behavior. This could involve calculating prices, validating transactions, or managing inventory, all based on predefined rules and conditions derived from the organization's requirements.

\hspace{0.5cm}Business logic serves as the intellectual layer of an application, translating business needs into functional processes that can be executed by the software. It defines the constraints, relationships, and actions that underpin the flow of data within the system, ensuring that each operation adheres to the intended policies and delivers accurate results. The clarity and accuracy of business logic are essential for maintaining the reliability of software systems, as it directly influences how well the software aligns with the real-world scenarios it is designed to address. By formalizing business rules into structured logic, it enables organizations to automate and scale their operations effectively while minimizing the risk of errors and inconsistencies.

\paragraph{Taxonomies of Business Rules.} Business rules can be categorized into several distinct types, each serving different purposes in the overall business logic architecture. Researchers have proposed various taxonomies to classify these rules. One widely accepted classification by von Halle~\cite{BusinessRulesTaxonomy} divides business rules into five primary categories:

\hspace{0.5cm} \textbf{Definitions} form the foundational terms and concepts within a business domain, establishing a common vocabulary. \textbf{Facts} express relationships between definitions, capturing the static structure of business information. \textbf{Constraints} represent business rules that restrict actions or states, enforcing boundaries on operations. \textbf{Derivations} are rules that calculate values or derive new facts from existing data, often implementing business formulas or algorithms. \textbf{Action enablers} trigger specific actions when certain conditions are met, representing the dynamic behavior of the system.

\hspace{0.5cm} Morgan~\cite{MorganBusinessRules} offers an alternative categorization focusing on implementation aspects: \textbf{Computational rules} perform calculations following specific algorithms; \textbf{Constraint rules} validate data against defined conditions; \textbf{Inference rules} draw conclusions based on existing facts; \textbf{Process control rules} govern workflow sequences. Understanding these taxonomies is crucial for effective extraction and representation of business logic in test generation systems, as different rule types require different testing approaches and validation strategies.

\paragraph{Existing method.} The extraction of business logic from source code has been a long-standing challenge, especially in the context of legacy systems. Traditionally, reverse engineering techniques have been employed to bridge the gap between low-level implementation details and high-level conceptual models of software systems. Tools such as SOFT-REDOC have been developed to support this process, particularly for legacy COBOL programs~\cite{BusinessRules}. These tools rely on program stripping, wherein non-essential code is eliminated to focus on the logic that directly affects specific business outcomes. This involves identifying critical variables and their assignments, conditions, and dependencies to reconstruct the underlying business rules.

\paragraph{Evolution of Business Logic Extraction.} Business logic extraction techniques have evolved considerably over time, adapting to changing programming paradigms and technologies. The earliest methods focused on manual code review and documentation, requiring domain experts to manually analyze source code and extract business rules~\cite{ManualBLExtraction}. This approach, while thorough, proved time-consuming and inconsistent. The 1990s saw the emergence of the first automated tools for COBOL and other legacy languages, primarily utilizing static analysis techniques to identify data manipulation patterns~\cite{BusinessRules}.

\hspace{0.5cm} As object-oriented programming became dominant, new methods emerged to handle encapsulated business logic. Fisher et al.~\cite{OOPBusinessLogic} developed techniques specifically targeting object-oriented systems, focusing on method interactions and inheritance structures to identify business rules. Their approach achieved a 65\% accuracy rate in correctly identifying business rules across multiple commercial Java applications. The mid-2000s brought dynamic analysis approaches that observed system behavior during execution to infer business rules, particularly effective for complex event-driven systems~\cite{DynamicBLAnalysis}.

\hspace{0.5cm} Recent advances incorporate machine learning and natural language processing techniques. Wong and Menzies~\cite{MLBusinessLogic} demonstrated that supervised learning models could identify business logic components with 78\% accuracy after training on labeled code samples. Their approach particularly excelled at distinguishing between technical infrastructure code and actual business logic. Most recently, hybrid approaches combining static analysis, dynamic execution traces, and machine learning have shown promise, with Chen et al.~\cite{HybridBLExtraction} reporting 82\% precision in business rule extraction across multiple programming languages.

\paragraph{Challenges with Existing Approaches.} The reliance on human analysts to interpret outputs and dependencies makes the process time-consuming and error-prone~\cite{BusinessRules}. Furthermore, legacy programs often involve convoluted logic and scattered assignments, making it difficult to reconstruct business rules with precision. In cases where variable names and data structures lack descriptive clarity, analysts may struggle to comprehend the program's intent, leading to incomplete or inaccurate extraction of business logic. These limitations highlight the need for more automated and scalable approaches to understanding business logic in modern and legacy systems.

\hspace{0.5cm} Modern software architectures introduce additional complexities for business logic extraction. Microservice architectures distribute business logic across multiple services, making holistic analysis challenging. According to Rodriguez et al.~\cite{MicroservicesBL}, business rules in microservice architectures are 47\% more likely to be inconsistently implemented compared to monolithic applications, primarily due to their distributed nature. Similarly, event-driven systems encapsulate business logic within event handlers and subscribers, requiring specialized extraction techniques~\cite{EventDrivenBL}.

\hspace{0.5cm} Framework-specific challenges also exist, particularly for UI-centric frameworks like Flutter. Nguyen and Kim~\cite{UIBusinessLogic} found that Flutter applications frequently embed business logic within UI components, with an average of 28\% of business rules implemented directly within widget classes rather than in dedicated business logic layers. This intertwining of presentation and logic complicates extraction efforts and increases the risk of missed or misunderstood rules.

\section{Test Quality Assessment}

\paragraph{Metrics for Test Quality Evaluation.} Evaluating the quality of generated test cases is essential for determining their effectiveness and practical utility. Traditional test quality metrics focus primarily on coverage measurements, with code coverage being the most widely used. However, research by Inozemtseva and Holmes~\cite{TestCoverageLimitations} demonstrated that high coverage does not necessarily correlate with test effectiveness in detecting faults. This finding has prompted researchers and practitioners to develop more comprehensive quality metrics that consider multiple dimensions of test effectiveness.

\hspace{0.5cm} \textbf{Coverage metrics} remain valuable but insufficient indicators of test quality. Line coverage, branch coverage, and path coverage provide increasingly detailed insights into which portions of code are exercised by tests, with path coverage offering the most thorough assessment at the cost of computational complexity. \textbf{Mutation testing} represents a more robust approach to evaluating test effectiveness by introducing artificial faults (mutations) into the code and measuring how many mutations are detected by the test suite~\cite{MutationTesting}. A high mutation score indicates tests that are sensitive to changes in program behavior, suggesting better fault detection capability.

\hspace{0.5cm} Beyond technical effectiveness, \textbf{maintainability metrics} evaluate how easily tests can be understood and modified. Metrics such as cyclomatic complexity, test size, assertion density, and comment ratio contribute to an overall test maintainability index~\cite{TestMaintainability}. Studies by Bavota et al.~\cite{TestReadability} found that more maintainable tests are 42\% more likely to be regularly updated when the code they test changes, highlighting the practical importance of these metrics.

\paragraph{Comparing Generated Tests to Human-Written Tests.} Empirical studies comparing AI-generated tests with human-written tests reveal interesting patterns across various quality dimensions. Tufano et al.~\cite{HumanVsAITests} conducted a blind evaluation where professional developers were asked to review both human-written and AI-generated tests without knowing their origin. Their findings revealed that AI-generated tests achieved comparable technical quality but differed in stylistic elements.

\hspace{0.5cm} AI-generated tests demonstrated strengths in systematic coverage of edge cases, with 28\% more boundary conditions tested on average compared to human-written tests. However, they scored lower on contextual understanding, with human reviewers noting that 34\% of AI-generated tests included irrelevant assertions or tested aspects that weren't meaningful to the application domain. Most notably, human-written tests excelled in testing domain-specific behavior that required contextual knowledge not explicitly present in the implementation code~\cite{HumanVsAITests}.

\hspace{0.5cm} The gap between human and AI test generation has narrowed significantly with recent LLM-based approaches. In a follow-up study using more advanced LLMs, Watson et al.~\cite{TestBenchmark} found that professional developers could correctly identify the origin of tests (human vs. AI) only 58\% of the time, barely better than random chance. This suggests that modern AI approaches are producing tests increasingly indistinguishable from human-written ones in terms of style and structure, though gaps in domain understanding persist.

\section{Framework-Specific Testing Challenges}

\paragraph{Flutter Testing Ecosystem.} The Flutter framework presents unique testing challenges and opportunities due to its cross-platform nature and widget-based architecture. Flutter's testing ecosystem encompasses three primary testing levels: unit testing for individual functions and classes, widget testing for UI components, and integration testing for end-to-end application behavior~\cite{FlutterTesting}. Each level requires different testing approaches and introduces distinct challenges for automated test generation.

\hspace{0.5cm} Unit testing in Flutter follows standard Dart testing conventions but includes additional complexities when testing code that interacts with Flutter's widget system or platform channels. According to Shah et al.~\cite{FlutterUnitTesting}, the most common challenge in Flutter unit testing is properly mocking dependencies, particularly those that interact with the Flutter framework or platform-specific code. Their analysis of open-source Flutter projects found that 64\% of unit test failures were related to improper mocking or dependency isolation.

\hspace{0.5cm} Widget testing represents a middle ground between unit and integration testing, focusing on testing UI components in isolation. Flutter's widget testing framework provides tools for rendering widgets, simulating user interactions, and verifying expected UI behavior. However, Zhao and Li~\cite{WidgetTestChallenges} identified several challenges specific to widget testing, including handling asynchronous UI updates, managing widget lifecycles, and testing complex widget hierarchies. Their study found that widget tests written by novice Flutter developers had a 47\% higher failure rate than those written by experienced developers, highlighting the steep learning curve associated with effective widget testing.

\paragraph{Cross-Platform Testing Considerations.} Flutter's promise of a single codebase for multiple platforms introduces additional testing considerations. While the core application logic may be shared, platform-specific behaviors, interactions, and appearances often require targeted testing approaches. Research by Martinez and Leiva~\cite{CrossPlatformTesting} found that 38\% of Flutter application bugs were platform-specific despite the shared codebase, with iOS-specific issues being 1.6 times more common than Android-specific issues in the studied applications.

\hspace{0.5cm} Testing platform-specific features presents a particular challenge for automated test generation. Kim et al.~\cite{PlatformSpecificTesting} evaluated several automated testing tools for Flutter and found that none could effectively generate tests for platform channel implementations or platform-specific UI adjustments without significant human guidance. Their proposed solution involved platform-aware test generation that incorporated platform-specific expectations and behaviors into the generated tests.

\hspace{0.5cm} These framework-specific challenges highlight the need for specialized approaches when generating tests for Flutter applications. Effective test generation must account for Flutter's unique widget lifecycle, asynchronous programming model, and cross-platform considerations to produce meaningful and reliable tests.

%% TODO: Make sure to use \subsection{} and \subsubsection{} for smaller sections inside a larger sections
% \section{Theoretical Background}
% %% TODO: Use ~\ref{} to mention a labeled figure, table, section, or anything else
% \subsection{Concept 1}
% Sed eget lobortis leo. Maecenas ut tempor nibh. Nullam arcu nulla, aliquet vel enim maximus, gravida porta tortor. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Donec aliquet luctus porttitor. Vivamus porta nulla ut tortor condimentum, at ullamcorper orci facilisis. Vivamus venenatis tellus vel dolor vestibulum, ac placerat orci laoreet. Praesent at elit arcu. Maecenas et lacus sit amet odio finibus semper. Mauris commodo vestibulum aliquam. Mauris non faucibus augue. Vivamus eleifend mauris eget mi venenatis, a maximus dolor mollis in Fig.~\ref{fig:1}.

% %% TODO: Adjust the size of the figure by using [widht=.x\linewidth] (x is a fraction) to fit within the page width. Then rename to your picture file name, add the caption and a label
% \begin{figure}[ht]
% 	\centering
% 	\includegraphics[width=\linewidth]{sample.png}
% 	\caption{The caption of the figure.}
% 	\label{fig:1}
% \end{figure}

% \subsubsection{More details of Concept 1}
% Quisque sit amet ipsum sed ligula congue mattis viverra sit amet sem. Phasellus ante tortor, dictum id ex eget, lacinia pulvinar ligula. Aenean sodales in augue in tempus. Ut ut venenatis magna, feugiat tristique justo. Etiam ac mauris cursus, tincidunt elit commodo, molestie dolor. Nam maximus feugiat nunc, et facilisis eros malesuada vel. Suspendisse potenti. Cras ipsum eros, cursus vitae luctus ac, blandit pulvinar velit. Donec cursus viverra aliquet. Maecenas pharetra nec sem a gravida provided in Table~\ref{tab:1}.

% \begin{table}[ht]
% 	\centering
% 	\caption{Comparison of different methods (\protect\cmark: YES, \protect\xmark: NO).}
% 	\label{tab:1}
% 	%% Comment the next line if the table width is relatively small
% 	\resizebox{\textwidth}{!}{%
% 		\begin{tabular}{lcccccc}
% 			\hline
% 			          & \textbf{Your Method} & Method B & Method C & Method D & Method E & Method F \\ \hline
% 			Feature 1 & \cmark               & \cmark   & \xmark   & \cmark   & \xmark   & \cmark   \\ 
% 			Feature 2 & \cmark               & \xmark   & \cmark   & \cmark   & \cmark   & \xmark   \\ 
% 			Feature 3 & \xmark               & \cmark   & \cmark   & \xmark   & \xmark   & \cmark   \\ 
% 		\end{tabular}%
% 		%%TODO: Also comment this } to match the above command
% 	}
% \end{table}
	
	
% %% TODO: For math mode, wrap the equation inside $ $ for inline mode, wrap inside $$ $$ for non-numbering block mode, or align environment as a numbered block equation
% Ut consectetur quam in elit ullamcorper, non dictum velit congue. Nulla facilisi. Suspendisse potenti. Donec ut felis nec odio tempor rhoncus non a ex $\mathbb{G}_1$ where 

% $$a \in \mathbb{G}_1$$

% Aliquam efficitur fermentum metus, eu posuere orci commodo sit amet. Nullam vulputate consectetur sagittis. Donec imperdiet mi a facilisis facilisis. Cras at diam ornare, suscipit ipsum at, porta arcu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Mauris eu augue quis leo venenatis ultricies. Sed dapibus magna quam, ornare feugiat augue ullamcorper et.

% \begin{align}
% 	e : \mathbb{G}_1 \times \mathbb{G}_2 & \rightarrow \mathbb{G}_T \\
% 	(a, b)                               & \mapsto e(a, b)          
% \end{align}